{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51153c19",
   "metadata": {},
   "source": [
    "# Modeling Notebook\n",
    "\n",
    "End-to-end notebook: feature engineering, scaling, training (RandomForest + XGBoost), evaluation, and model export.\n",
    "\n",
    "**Paths and outputs:**\n",
    "- Input: `cleaned_engineered_data.csv` in the data folder\n",
    "- Outputs: `walmart_sales_predictions.csv`, `rf_model.pkl`, `xgb_model.pkl`, `scaler.pkl`\n",
    "\n",
    "Run cells top â†’ bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d29fa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WALMART SALES FORECASTING - MODEL TRAINING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WALMART SALES FORECASTING - MODEL TRAINING\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131b4650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Loading data...\n",
      "âœ“ Data loaded: 10000 rows, 24 columns\n",
      "âœ“ Columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Size', 'IsHoliday', 'Month', 'Week', 'Year', 'Day', 'DayOfWeek', 'Weekly_Sales_Rolling4', 'Sales_vs_StoreMean', 'Type_B', 'Type_C']\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: LOAD DATA\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 1] Loading data...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"data/cleaned_engineered_data_cleaned.csv\")\n",
    "    print(f\"âœ“ Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(f\"âœ“ Columns: {list(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ERROR: File 'data/cleaned_engineered_data_cleaned.csv' not found!\")\n",
    "    print(\"Please ensure the data file exists in the 'data' folder.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de2209b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Feature Engineering...\n",
      "âœ“ Date features created: Year, Month, Day, DayOfWeek, Week\n",
      "âœ“ Missing values filled in 1 numeric columns\n",
      "âœ“ Categorical columns found: None\n",
      "âœ“ Store column converted to integer\n",
      "âœ“ Dept column converted to integer\n",
      "âœ“ Final feature count: 23 columns\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 2] Feature Engineering...\")\n",
    "\n",
    "# Date features (if Date column exists)\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['Week'] = df['Date'].dt.isocalendar().week\n",
    "    df.drop(columns=['Date'], inplace=True)\n",
    "    print(\"âœ“ Date features created: Year, Month, Day, DayOfWeek, Week\")\n",
    "else:\n",
    "    print(\"âš  No 'Date' column found - skipping date feature creation\")\n",
    "\n",
    "# Fill missing values in numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "missing_count = 0\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        missing_count += 1\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"âœ“ Missing values filled in {missing_count} numeric columns\")\n",
    "else:\n",
    "    print(\"âœ“ No missing values found\")\n",
    "\n",
    "# Check for categorical columns that need encoding\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"âœ“ Categorical columns found: {categorical_cols if categorical_cols else 'None'}\")\n",
    "\n",
    "# IMPORTANT: We keep Store and Dept as NUMERIC, not one-hot encode them\n",
    "# This is crucial for the UI to work properly\n",
    "if 'Store' in df.columns:\n",
    "    df['Store'] = df['Store'].astype(int)\n",
    "    print(\"âœ“ Store column converted to integer\")\n",
    "\n",
    "if 'Dept' in df.columns:\n",
    "    df['Dept'] = df['Dept'].astype(int)\n",
    "    print(\"âœ“ Dept column converted to integer\")\n",
    "\n",
    "# Handle other categorical columns (if any) - like Type\n",
    "if 'Type' in df.columns and df['Type'].dtype == 'object':\n",
    "    # Keep Type_B and Type_C as they are (already encoded)\n",
    "    df.drop(columns=['Type'], inplace=True, errors='ignore')\n",
    "    print(\"âœ“ Removed 'Type' column (Type_B and Type_C already encoded)\")\n",
    "\n",
    "print(f\"âœ“ Final feature count: {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b41d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Preparing features and target...\n",
      "âœ“ Features (X): (10000, 22)\n",
      "âœ“ Target (y): (10000,)\n",
      "\n",
      "âœ“ Feature columns (22):\n",
      "   1. Store\n",
      "   2. Dept\n",
      "   3. Temperature\n",
      "   4. Fuel_Price\n",
      "   5. MarkDown1\n",
      "   6. MarkDown2\n",
      "   7. MarkDown3\n",
      "   8. MarkDown4\n",
      "   9. MarkDown5\n",
      "   10. CPI\n",
      "   11. Unemployment\n",
      "   12. Size\n",
      "   13. IsHoliday\n",
      "   14. Month\n",
      "   15. Week\n",
      "   16. Year\n",
      "   17. Day\n",
      "   18. DayOfWeek\n",
      "   19. Weekly_Sales_Rolling4\n",
      "   20. Sales_vs_StoreMean\n",
      "   21. Type_B\n",
      "   22. Type_C\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: PREPARE FEATURES AND TARGET\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 3] Preparing features and target...\")\n",
    "\n",
    "target_col = 'Weekly_Sales'\n",
    "\n",
    "if target_col not in df.columns:\n",
    "    print(f\"âŒ ERROR: Target column '{target_col}' not found in dataset!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "    exit(1)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"âœ“ Features (X): {X.shape}\")\n",
    "print(f\"âœ“ Target (y): {y.shape}\")\n",
    "print(f\"\\nâœ“ Feature columns ({len(X.columns)}):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457af111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Train-Test Split...\n",
      "âœ“ Training set: 8000 samples\n",
      "âœ“ Test set: 2000 samples\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: TRAIN-TEST SPLIT\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 4] Train-Test Split...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"âœ“ Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "126b0fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Training Random Forest Model...\n",
      "â³ This may take a few minutes...\n",
      "âœ“ Model training complete!\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: TRAIN RANDOM FOREST MODEL\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 5] Training Random Forest Model...\")\n",
    "print(\"â³ This may take a few minutes...\")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"âœ“ Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d4136ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5b] Training XGBoost Model...\n",
      "â³ This may take a few minutes...\n",
      "âœ“ XGBoost training complete!\n"
     ]
    }
   ],
   "source": [
    "# STEP 5b: TRAIN XGBOOST MODEL\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 5b] Training XGBoost Model...\")\n",
    "print(\"â³ This may take a few minutes...\")\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"âœ“ XGBoost training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880eba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Model Evaluation...\n",
      "\n",
      "TRAINING SET Performance:\n",
      "  MAE:  $21.62\n",
      "  RMSE: $215.23\n",
      "  RÂ²:   0.9999\n",
      "  MAPE: inf%\n",
      "\n",
      "TEST SET Performance:\n",
      "  MAE:  $35.91\n",
      "  RMSE: $276.21\n",
      "  RÂ²:   0.9998\n",
      "  MAPE: inf%\n",
      "\n",
      "[Feature Importance] Top 20 Features:\n",
      "              Feature   Importance\n",
      "   Sales_vs_StoreMean 9.996272e-01\n",
      "                Store 1.143402e-04\n",
      "                 Size 1.041665e-04\n",
      "                 Dept 6.198808e-05\n",
      "         Unemployment 5.267525e-05\n",
      "Weekly_Sales_Rolling4 1.157001e-05\n",
      "          Temperature 8.438486e-06\n",
      "                  CPI 7.485981e-06\n",
      "                  Day 4.088504e-06\n",
      "                 Week 3.046947e-06\n",
      "           Fuel_Price 2.169864e-06\n",
      "                Month 1.340863e-06\n",
      "            IsHoliday 8.287108e-07\n",
      "                 Year 1.716667e-07\n",
      "            MarkDown1 1.263021e-07\n",
      "            MarkDown5 8.782040e-08\n",
      "            MarkDown2 8.380953e-08\n",
      "            MarkDown4 7.958106e-08\n",
      "            MarkDown3 7.823684e-08\n",
      "            DayOfWeek 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: EVALUATE MODEL\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 6] Model Evaluation...\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Metrics function\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Performance:\")\n",
    "    print(f\"  MAE:  ${mae:,.2f}\")\n",
    "    print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"  RÂ²:   {r2:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, \"TRAINING SET\")\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, \"TEST SET\")\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\n[Feature Importance] Top 20 Features:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9688064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Performance:\")\n",
    "    print(f\"  MAE:  ${mae:,.2f}\")\n",
    "    print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"  RÂ²:   {r2:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06ceefb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6b] Evaluating XGBoost Model...\n",
      "\n",
      "TRAINING SET (XGB) Performance:\n",
      "  MAE:  $65.50\n",
      "  RMSE: $96.24\n",
      "  RÂ²:   1.0000\n",
      "  MAPE: inf%\n",
      "\n",
      "TEST SET (XGB) Performance:\n",
      "  MAE:  $156.99\n",
      "  RMSE: $408.08\n",
      "  RÂ²:   0.9996\n",
      "  MAPE: inf%\n"
     ]
    }
   ],
   "source": [
    "# STEP 6b: Evaluate XGBoost Model\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 6b] Evaluating XGBoost Model...\")\n",
    "\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "train_metrics_xgb = evaluate_model(y_train, y_train_pred_xgb, \"TRAINING SET (XGB)\")\n",
    "test_metrics_xgb = evaluate_model(y_test, y_test_pred_xgb, \"TEST SET (XGB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f50d1bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ XGBoost metrics saved: model_metrics_xgb.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SAVE XGBOOST METRICS (REQUIRED FOR STREAMLIT APP)\n",
    "# ============================================================\n",
    "\n",
    "metrics_xgb_df = pd.DataFrame({\n",
    "    \"Metric\": [\"MAE\", \"RMSE\", \"R2\", \"MAPE\"],\n",
    "    \"Train\": [\n",
    "        train_metrics_xgb[\"MAE\"],\n",
    "        train_metrics_xgb[\"RMSE\"],\n",
    "        train_metrics_xgb[\"R2\"],\n",
    "        train_metrics_xgb[\"MAPE\"]\n",
    "    ],\n",
    "    \"Test\": [\n",
    "        test_metrics_xgb[\"MAE\"],\n",
    "        test_metrics_xgb[\"RMSE\"],\n",
    "        test_metrics_xgb[\"R2\"],\n",
    "        test_metrics_xgb[\"MAPE\"]\n",
    "    ]\n",
    "})\n",
    "\n",
    "metrics_xgb_df.to_csv(\"model_metrics_xgb.csv\", index=False)\n",
    "print(\"âœ“ XGBoost metrics saved: model_metrics_xgb.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bfeae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ XGBoost model saved: xgb_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# STEP 7b: Save XGBoost Model\n",
    "# ============================================================\n",
    "joblib.dump(xgb_model, 'xgb_model.pkl')\n",
    "print(\"âœ“ XGBoost model saved: xgb_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b733f7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Saving model and artifacts...\n",
      "âœ“ Model saved: rf_model.pkl\n",
      "âœ“ Feature names saved: input_features.pkl\n",
      "âœ“ Feature importance saved: feature_importance.csv\n",
      "âœ“ Model metrics saved: model_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: SAVE MODEL AND ARTIFACTS\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 7] Saving model and artifacts...\")\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(rf_model, 'rf_model.pkl')\n",
    "print(\"âœ“ Model saved: rf_model.pkl\")\n",
    "\n",
    "# Save feature names (column order is critical!)\n",
    "feature_names = list(X.columns)\n",
    "joblib.dump(feature_names, 'input_features.pkl')\n",
    "print(\"âœ“ Feature names saved: input_features.pkl\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "print(\"âœ“ Feature importance saved: feature_importance.csv\")\n",
    "\n",
    "# Save model metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'RMSE', 'R2', 'MAPE'],\n",
    "    'Train': [train_metrics['MAE'], train_metrics['RMSE'], train_metrics['R2'], train_metrics['MAPE']],\n",
    "    'Test': [test_metrics['MAE'], test_metrics['RMSE'], test_metrics['R2'], test_metrics['MAPE']]\n",
    "})\n",
    "metrics_df.to_csv('model_metrics.csv', index=False)\n",
    "print(\"âœ“ Model metrics saved: model_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77802ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 8] Creating store reference statistics...\n",
      "âœ“ Added rolling statistics\n",
      "âœ“ Store reference stats saved: store_reference_stats.csv (2 stores)\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: STORE REFERENCE STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 8] Creating store reference statistics...\")\n",
    "\n",
    "# Create aggregation dictionary\n",
    "agg_dict = {}\n",
    "\n",
    "if 'Store' in df.columns:\n",
    "    # Add Size if exists\n",
    "    if 'Size' in df.columns:\n",
    "        agg_dict['Size'] = 'first'\n",
    "    \n",
    "    # Add Type_B if exists\n",
    "    if 'Type_B' in df.columns:\n",
    "        agg_dict['Type_B'] = 'first'\n",
    "    \n",
    "    # Add Type_C if exists\n",
    "    if 'Type_C' in df.columns:\n",
    "        agg_dict['Type_C'] = 'first'\n",
    "    \n",
    "    # Create store stats\n",
    "    if agg_dict:\n",
    "        store_stats = df.groupby('Store').agg(agg_dict).reset_index()\n",
    "        \n",
    "        # Rename columns to match expected format\n",
    "        column_mapping = {\n",
    "            'Size': 'size',\n",
    "            'Type_B': 'type_b',\n",
    "            'Type_C': 'type_c'\n",
    "        }\n",
    "        store_stats.columns = ['Store'] + [column_mapping.get(col, col.lower()) for col in store_stats.columns[1:]]\n",
    "        \n",
    "        # Add rolling statistics if available\n",
    "        if 'Weekly_Sales_Rolling4' in df.columns:\n",
    "            rolling_stats = df.groupby('Store')['Weekly_Sales_Rolling4'].median().reset_index()\n",
    "            rolling_stats.columns = ['Store', 'rolling_median']\n",
    "            store_stats = store_stats.merge(rolling_stats, on='Store', how='left')\n",
    "            print(\"âœ“ Added rolling statistics\")\n",
    "        \n",
    "        store_stats.to_csv('store_reference_stats.csv', index=False)\n",
    "        print(f\"âœ“ Store reference stats saved: store_reference_stats.csv ({len(store_stats)} stores)\")\n",
    "    else:\n",
    "        print(\"âš  Warning: No store characteristics found, skipping store stats\")\n",
    "else:\n",
    "    print(\"âš  Warning: No 'Store' column found, skipping store stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698b60f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 9] Verification - Testing Prediction...\n",
      "\n",
      "Test Sample Prediction:\n",
      "  Actual Sales:    $13,557.11\n",
      "  Predicted Sales: $13,554.48\n",
      "  Difference:      $2.63\n",
      "  Error %:         0.02%\n",
      "\n",
      "Sample Input Features:\n",
      "  Store: 1\n",
      "  Dept: 49\n",
      "  Temperature: 91.65\n",
      "  Fuel_Price: 3.684\n",
      "  MarkDown1: 0.0\n",
      "  MarkDown2: 0.0\n",
      "  MarkDown3: 0.0\n",
      "  MarkDown4: 0.0\n",
      "  MarkDown5: 0.0\n",
      "  CPI: 215.544618\n",
      "  ... and 12 more features\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "âœ“ Model Type: Random Forest Regressor\n",
      "âœ“ Number of Trees: 300\n",
      "âœ“ Max Depth: 15\n",
      "âœ“ Features: 22\n",
      "âœ“ Training Samples: 8000\n",
      "âœ“ Test Samples: 2000\n",
      "\n",
      "ğŸ“Š Model Performance:\n",
      "  â€¢ Test RÂ² Score: 0.9998\n",
      "  â€¢ Test MAE: $35.91\n",
      "  â€¢ Test RMSE: $276.21\n",
      "  â€¢ Test MAPE: inf%\n",
      "\n",
      "ğŸ“ Files Generated:\n",
      "  1. âœ“ rf_model.pkl - Trained Random Forest model\n",
      "  2. âœ“ input_features.pkl - Feature names list\n",
      "  3. âœ“ feature_importance.csv - Feature importance rankings\n",
      "  4. âœ“ model_metrics.csv - Model performance metrics\n",
      "  5. âœ“ store_reference_stats.csv - Store-level statistics\n",
      "\n",
      "ğŸš€ Ready for deployment!\n",
      "   Run: streamlit run app.py\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# STEP 9: TEST PREDICTION (VERIFICATION)\n",
    "# ============================================================\n",
    "print(\"\\n[STEP 9] Verification - Testing Prediction...\")\n",
    "\n",
    "# Get first row of test data\n",
    "test_sample = X_test.iloc[0:1].copy()\n",
    "actual_value = y_test.iloc[0]\n",
    "predicted_value = rf_model.predict(test_sample)[0]\n",
    "\n",
    "print(f\"\\nTest Sample Prediction:\")\n",
    "print(f\"  Actual Sales:    ${actual_value:,.2f}\")\n",
    "print(f\"  Predicted Sales: ${predicted_value:,.2f}\")\n",
    "print(f\"  Difference:      ${abs(actual_value - predicted_value):,.2f}\")\n",
    "print(f\"  Error %:         {abs(actual_value - predicted_value) / actual_value * 100:.2f}%\")\n",
    "\n",
    "# Show sample input values\n",
    "print(f\"\\nSample Input Features:\")\n",
    "for col in test_sample.columns[:10]:  # Show first 10 features\n",
    "    print(f\"  {col}: {test_sample[col].values[0]}\")\n",
    "if len(test_sample.columns) > 10:\n",
    "    print(f\"  ... and {len(test_sample.columns) - 10} more features\")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nâœ“ Model Type: Random Forest Regressor\")\n",
    "print(f\"âœ“ Number of Trees: 300\")\n",
    "print(f\"âœ“ Max Depth: 15\")\n",
    "print(f\"âœ“ Features: {len(feature_names)}\")\n",
    "print(f\"âœ“ Training Samples: {len(X_train)}\")\n",
    "print(f\"âœ“ Test Samples: {len(X_test)}\")\n",
    "print(f\"\\nğŸ“Š Model Performance:\")\n",
    "print(f\"  â€¢ Test RÂ² Score: {test_metrics['R2']:.4f}\")\n",
    "print(f\"  â€¢ Test MAE: ${test_metrics['MAE']:,.2f}\")\n",
    "print(f\"  â€¢ Test RMSE: ${test_metrics['RMSE']:,.2f}\")\n",
    "print(f\"  â€¢ Test MAPE: {test_metrics['MAPE']:.2f}%\")\n",
    "print(f\"\\nğŸ“ Files Generated:\")\n",
    "print(\"  1. âœ“ rf_model.pkl - Trained Random Forest model\")\n",
    "print(\"  2. âœ“ input_features.pkl - Feature names list\")\n",
    "print(\"  3. âœ“ feature_importance.csv - Feature importance rankings\")\n",
    "print(\"  4. âœ“ model_metrics.csv - Model performance metrics\")\n",
    "print(\"  5. âœ“ store_reference_stats.csv - Store-level statistics\")\n",
    "print(\"\\nğŸš€ Ready for deployment!\")\n",
    "print(\"   Run: streamlit run app.py\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
